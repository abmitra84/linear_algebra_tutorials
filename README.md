# What is tensor and how is it constructed?
https://youtu.be/f5liqUk0ZTw


# 3 blue 1 brown's awesome videos on visual intuition of linear algebraic transformations: 
https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab

# Eigenvector and Eigenvalues:
  1. https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PL_w8oSr1JpVCZ5pKXHKz6PkjGCbPbSBYv&index=14 by 3b1b
  2. https://medium.com/@jonathan_hui/machine-learning-linear-algebra-eigenvalue-and-eigenvector-f8d0493564c9 little complex yet very useful write up

# PCA:
  1. https://www.youtube.com/watch?v=FgakZw6K1QQ by Josh Starmer
  2. https://builtin.com/data-science/step-step-explanation-principal-component-analysis 
      Think why would data  is so important for PCA. It is beyond what people teach you generally. You basically can't get the benefit of SVD or usage of Covariance matrix right
  3. http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf Little more detailed.
  4. https://arxiv.org/pdf/1404.1100.pdf far more detailed
  5. Good discussion thread: https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component
  6. https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491 why SVD over Eigendecomposition
  7. Some more stuff on eigenvectors: https://pathmind.com/wiki/eigenvector
